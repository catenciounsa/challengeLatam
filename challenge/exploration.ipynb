{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba1ec38",
   "metadata": {},
   "source": [
    "# 0. Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from ultralytics import YOLO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_YAML_PATH = \"../data/data.yaml\"\n",
    "\n",
    "assert os.path.exists(DATA_YAML_PATH), f\"data.yaml not found in {DATA_YAML_PATH}\"\n",
    "\n",
    "with open(DATA_YAML_PATH, \"r\") as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "\n",
    "data_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = data_cfg.get(\"train\")\n",
    "val_dir   = data_cfg.get(\"val\")\n",
    "test_dir  = data_cfg.get(\"test\", None)\n",
    "class_names = data_cfg.get(\"names\", [])\n",
    "nc = int(data_cfg.get(\"nc\", len(class_names)))\n",
    "\n",
    "print(\"Train images dir:\", train_dir)\n",
    "print(\"Val images dir  :\", val_dir)\n",
    "print(\"Test images dir :\", test_dir)\n",
    "print(\"Classes (nc)     :\", nc)\n",
    "print(\"Classes name:\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994b156",
   "metadata": {},
   "source": [
    "# 1. Data Analysis: First Sight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4690c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_label_paths(img_dir: str) -> list:\n",
    "    img_paths = []\n",
    "    for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\"):\n",
    "        img_paths.extend(glob.glob(os.path.join(img_dir, ext)))\n",
    "    label_paths = []\n",
    "    for ip in img_paths:\n",
    "        lp = ip.replace(os.sep + \"images\" + os.sep, os.sep + \"labels\" + os.sep)\n",
    "        lp = os.path.splitext(lp)[0] + \".txt\"\n",
    "        label_paths.append((ip, lp))\n",
    "    return label_paths\n",
    "\n",
    "def read_yolo_labels(label_path: str):\n",
    "    if not os.path.exists(label_path):\n",
    "        return []\n",
    "    rows = []\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                cls_id = int(float(parts[0]))\n",
    "                cx, cy, w, h = map(float, parts[1:5])\n",
    "                rows.append((cls_id, cx, cy, w, h))\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_split(img_dir: str, class_names: list):\n",
    "    pairs = yolo_label_paths(img_dir)\n",
    "    per_class = Counter()\n",
    "    objs_per_image = []\n",
    "    areas = []\n",
    "\n",
    "    for _, lbl_path in pairs:\n",
    "        labels = read_yolo_labels(lbl_path)\n",
    "        objs_per_image.append(len(labels))\n",
    "        for (cls_id, cx, cy, w, h) in labels:\n",
    "            per_class[cls_id] += 1\n",
    "            areas.append(w * h)\n",
    "\n",
    "    df_classes = pd.DataFrame({\n",
    "        \"class_id\": list(per_class.keys()),\n",
    "        \"count\": list(per_class.values())\n",
    "    })\n",
    "    df_classes[\"class_name\"] = df_classes[\"class_id\"].apply(lambda i: class_names[i] if i < len(class_names) else str(i))\n",
    "\n",
    "    df_objs = pd.DataFrame({\"objects_per_image\": objs_per_image})\n",
    "    df_areas = pd.DataFrame({\"bbox_area_norm\": areas})\n",
    "\n",
    "    return df_classes.sort_values(\"count\", ascending=False), df_objs, df_areas\n",
    "\n",
    "df_classes_train, df_objs_train, df_areas_train = analyze_split(train_dir, class_names)\n",
    "df_classes_train.head(), df_objs_train.describe(), df_areas_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c49472",
   "metadata": {},
   "source": [
    "### How is the date distribuited?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(df_classes_train[\"class_name\"], df_classes_train[\"count\"])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Object count per class (train)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(df_objs_train[\"objects_per_image\"], bins=20)\n",
    "plt.title(\"Objects per image (train)\")\n",
    "plt.xlabel(\"# objects\")\n",
    "plt.ylabel(\"frecuency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(df_areas_train[\"bbox_area_norm\"], bins=30)\n",
    "plt.title(\"Normalized bbox area distribution (train)\")\n",
    "plt.xlabel(\"w*h (normalized)\")\n",
    "plt.ylabel(\"frecuency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_yolo_bbox(img, bbox, color=(0,255,0), thickness=2):\n",
    "    H, W = img.shape[:2]\n",
    "    cx, cy, bw, bh = bbox\n",
    "    x1 = int((cx - bw/2) * W)\n",
    "    y1 = int((cy - bh/2) * H)\n",
    "    x2 = int((cx + bw/2) * W)\n",
    "    y2 = int((cy + bh/2) * H)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\n",
    "    return img\n",
    "\n",
    "def visualize_samples(img_dir: str, class_names: list, n=4, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    pairs = yolo_label_paths(img_dir)\n",
    "    sample = rng.sample(pairs, min(n, len(pairs)))\n",
    "    fig, axes = plt.subplots(1, len(sample), figsize=(4*len(sample), 4))\n",
    "    if len(sample) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, (img_path, lbl_path) in zip(axes, sample):\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        labels = read_yolo_labels(lbl_path)\n",
    "        for (cls_id, cx, cy, w, h) in labels:\n",
    "            img = draw_yolo_bbox(img, (cx, cy, w, h))\n",
    "            name = class_names[cls_id] if cls_id < len(class_names) else str(cls_id)\n",
    "            ax.text(5, 15, name, color='yellow', bbox=dict(facecolor='black', alpha=0.5))\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(Path(img_path).name)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(train_dir, class_names, n=4, seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a7ba8",
   "metadata": {},
   "source": [
    "### What methods would you use to verify the reliability of the labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f52b6",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b470c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üîß HYPERPARAMETERS ‚Äî EXPERIMENTAL SETUP\n",
    "# ==============================\n",
    "\n",
    "# TODO: Fill in the hyperparameters based on your dataset analysis.\n",
    "# Justify your choices in the Markdown cell above.\n",
    "\n",
    "EPOCHS = ___\n",
    "IMGSZ  = ___\n",
    "BATCH  = ___\n",
    "DEVICE = \"cpu\"        # or \"cuda\" if available\n",
    "\n",
    "# Try YOLO11; if not available use YOLOv8\n",
    "weights_try = [\"yolo11n.pt\", \"yolov8n.pt\"]\n",
    "model = None\n",
    "for w in weights_try:\n",
    "    try:\n",
    "        model = YOLO(w)\n",
    "        print(\"Using:\", w)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {w}: {e}\")\n",
    "\n",
    "assert model is not None, \"Could not load a base model (yolo11n.pt / yolov8n.pt). Install ultralytics and make sure you have an active internet connection to download the weights.\"\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# üöÄ TRAINING ‚Äî BASELINE EXPERIMENT\n",
    "# ==============================\n",
    "# The results object contains metrics, charts, and run directory info.\n",
    "# Feel free to adjust and rerun with different hyperparameters.\n",
    "\n",
    "results = model.train(data=DATA_YAML_PATH, epochs=EPOCHS, imgsz=IMGSZ, batch=BATCH, device=DEVICE)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce46504",
   "metadata": {},
   "source": [
    "### ‚úèÔ∏è Discussion\n",
    "\n",
    "Explain:\n",
    "1. Why did you choose these hyperparameters?  \n",
    "2. How do they affect training time, GPU/CPU usage, and accuracy?  \n",
    "3. What would you try differently if you had more time or resources?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05165920",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20906a2",
   "metadata": {},
   "source": [
    "\n",
    "> üëâ **Task:** Evaluate your trained model using the validation set defined in `data.yaml`.\n",
    "\n",
    "Run the following cell to compute key performance metrics\n",
    "Then, summarize your results and provide your interpretation.\n",
    "\n",
    "**Guidelines for your analysis:**\n",
    "- **Quantitative metrics** \n",
    "- **Error analysis**\n",
    "- **Next steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20320b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Here your model metrics\n",
    "# TODO:\n",
    "# - Run model validation on the dataset below.\n",
    "# - Capture metrics and save a summary to artifacts/metrics_summary.json.\n",
    "# - Optionally, add visual analysis (PR curves, confusion matrix).\n",
    "\n",
    "# Put your model evaluation code here üëá\n",
    "\n",
    "metrics = model.val(data=DATA_YAML_PATH, imgsz=IMGSZ, device=DEVICE)\n",
    "try:\n",
    "    summary = {\n",
    "        \"metrics/mAP50-95(B)\": float(metrics.box.map if hasattr(metrics, \"box\") else getattr(metrics, \"map\", float(\"nan\"))),\n",
    "        \"metrics/mAP50(B)\"   : float(getattr(metrics, \"map50\", float(\"nan\"))),\n",
    "        \"nc\": int(nc),\n",
    "        \"classes\": class_names,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"imgsz\": IMGSZ,\n",
    "    }\n",
    "except Exception as e:\n",
    "    summary = {\"error\": str(e)}\n",
    "    \n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "with open(\"artifacts/metrics_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d460a",
   "metadata": {},
   "source": [
    "### ‚úèÔ∏è Metrics Interpretation and Analysis\n",
    "\n",
    "Provide a short written analysis here:\n",
    "\n",
    "1. **Quantitative Summary:**\n",
    "   - What are your `mAP50` and `mAP50-95` values?\n",
    "   - Which classes achieved the highest and lowest detection performance?\n",
    "\n",
    "2. **Qualitative Analysis:**\n",
    "   - Describe common failure cases (e.g., small objects missed, overlapping detections, background confusion).\n",
    "   - Were there any label quality issues or inconsistencies you observed?\n",
    "\n",
    "3. **Improvement Proposals:**\n",
    "   - Suggest at least two improvements (data augmentation, loss tuning, class balancing, etc.).\n",
    "   - How would you validate whether these changes actually help?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b895120",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SHOW_N = 4\n",
    "val_imgs = []\n",
    "for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
    "    val_imgs.extend(glob.glob(os.path.join(val_dir, ext)))\n",
    "val_imgs = val_imgs[:VAL_SHOW_N]\n",
    "\n",
    "pred = model.predict(source=val_imgs, imgsz=IMGSZ, conf=0.25)\n",
    "# Mostrar con matplotlib (usamos 'plot' de ultralytics para guardar)\n",
    "out_dir = \"runs/predict_display\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(val_imgs), figsize=(4*len(val_imgs), 4))\n",
    "if len(val_imgs) == 1:\n",
    "    axes = [axes]\n",
    "for ax, r in zip(axes, pred):\n",
    "    im = r.plot()  # numpy array con anotaciones\n",
    "    ax.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e065ca1",
   "metadata": {},
   "source": [
    "# 5. Export and deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_dir = Path(\"artifacts\")\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_ckpt = None\n",
    "for p in Path(\"runs/detect\").rglob(\"weights/best.pt\"):\n",
    "    best_ckpt = p\n",
    "    break\n",
    "\n",
    "if best_ckpt and best_ckpt.exists():\n",
    "    target = export_dir / \"model_best.pt\"\n",
    "    target.write_bytes(best_ckpt.read_bytes())\n",
    "    print(\"Wheight export to:\", target)\n",
    "else:\n",
    "    print(\"'best.pt' not found\")\n",
    "\n",
    "with open(export_dir / \"classes.json\", \"w\") as f:\n",
    "    json.dump({\"nc\": int(nc), \"names\": class_names}, f, indent=2)\n",
    "\n",
    "try:\n",
    "    _ = model.export(format=\"onnx\", imgsz=IMGSZ)\n",
    "    onnx_file = None\n",
    "    for p in Path(\".\").rglob(\"*.onnx\"):\n",
    "        onnx_file = p\n",
    "        break\n",
    "    if onnx_file:\n",
    "        (export_dir / \"model.onnx\").write_bytes(onnx_file.read_bytes())\n",
    "        print(\"ONNX export to:\", export_dir / \"model.onnx\")\n",
    "except Exception as e:\n",
    "    print(\"Export ONNX not available:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df39bb",
   "metadata": {},
   "source": [
    "# 6. TODOs (for the candidate)\n",
    "- [ ] Analyze the class imbalance and propose strategies (weighting, augmented sampling, focal loss).\n",
    "- [ ] Tune hyperparameters (epochs, image size, augmentations) to improve mAP.\n",
    "- [ ] Record key metrics and justify the final baseline.\n",
    "- [ ] Prepare all necessary artifacts in artifacts/ for the inference service (API)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc557447",
   "metadata": {},
   "source": [
    "# 7. Appendix ‚Äî Notes on relative paths\n",
    "- This notebook reads data.yaml and infers the paths to images/ and labels/ for train/, val/, and test/.\n",
    "- If you move data.yaml to another folder, adjust DATA_YAML_PATH.\n",
    "- If the dataset was downloaded from Roboflow, keep the standard YOLO folder structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
